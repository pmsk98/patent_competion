{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Abstract Keyword Extraction.ipynb","private_outputs":true,"provenance":[],"collapsed_sections":[],"machine_shape":"hm","authorship_tag":"ABX9TyO7kZHRQzZ9+0FMFpPWfqxz"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"s_MF_nx6PVsx"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"C_xQPlViPbxb"},"source":["import pandas as pd\n","import os\n","\n","os.chdir('/content/gdrive/My Drive/특허경진대회/')\n","os.getcwd()\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RuYH7MyuPb0k"},"source":["data = pd.read_csv('data.csv')\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"I_3avv1SPb5e"},"source":["abstract = data['abstract']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WoNXw1sZPb9S"},"source":["import re, os, string\n","import pandas as pd\n","\n","# Scikit-learn importings\n","from sklearn.feature_extraction.text import TfidfVectorizer"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"C4sJznjUPtm7"},"source":["def clean_text(text):\n","    \"\"\"Doc cleaning\"\"\"\n","    \n","    # Lowering text\n","    text = text.lower()\n","    \n","    # Removing punctuation\n","    text = \"\".join([c for c in text if c not in PUNCTUATION])\n","    \n","    # Removing whitespace and newlines\n","    text = re.sub('\\s+',' ',text)\n","    \n","    return text"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vf-IpAMsPyVC"},"source":["def sort_coo(coo_matrix):\n","    \"\"\"Sort a dict with highest score\"\"\"\n","    tuples = zip(coo_matrix.col, coo_matrix.data)\n","    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n","\n","def extract_topn_from_vector(feature_names, sorted_items, topn=10):\n","    \"\"\"get the feature names and tf-idf score of top n items\"\"\"\n","    \n","    #use only topn items from vector\n","    sorted_items = sorted_items[:topn]\n","\n","    score_vals = []\n","    feature_vals = []\n","    \n","    # word index and corresponding tf-idf score\n","    for idx, score in sorted_items:\n","        \n","        #keep track of feature name and its corresponding score\n","        score_vals.append(round(score, 3))\n","        feature_vals.append(feature_names[idx])\n","\n","    #create a tuples of feature, score\n","    results= {}\n","    for idx in range(len(feature_vals)):\n","        results[feature_vals[idx]]=score_vals[idx]\n","    \n","    return results"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"G4UV_M-UP03h"},"source":["def get_keywords(vectorizer, feature_names, doc):\n","    \"\"\"Return top k keywords from a doc using TF-IDF method\"\"\"\n","\n","    #generate tf-idf for the given document\n","    tf_idf_vector = vectorizer.transform([doc])\n","    \n","    #sort the tf-idf vectors by descending order of scores\n","    sorted_items=sort_coo(tf_idf_vector.tocoo())\n","\n","    #extract only TOP_K_KEYWORDS\n","    keywords=extract_topn_from_vector(feature_names,sorted_items,TOP_K_KEYWORDS)\n","    \n","    return list(keywords.keys())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qsjg5IcgP0ZC"},"source":["# Constants\n","PUNCTUATION = \"\"\"!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\"\"\" \n","TOP_K_KEYWORDS = 10 # top k number of keywords to retrieve in a ranked document"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-9WYX4N-P9Qr"},"source":["abstract = abstract.apply(clean_text)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nTO-bIK7QO-x"},"source":["corpora = abstract.to_list()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UnmmIEszQCEQ"},"source":["# Initializing TF-IDF Vectorizer with stopwords\n","vectorizer = TfidfVectorizer(smooth_idf=True, use_idf=True)\n","\n","# Creating vocab with our corpora\n","# Exlcluding first 10 docs for testing purpose\n","vectorizer.fit_transform(corpora[10::])\n","\n","# Storing vocab\n","feature_names = vectorizer.get_feature_names()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8NMRvFkyQLGC"},"source":["result = []\n","for doc in corpora[0:10]:\n","    df = {}\n","    df['full_text'] = doc\n","    df['top_keywords'] = get_keywords(vectorizer, feature_names, doc)\n","    result.append(df)\n","    \n","final = pd.DataFrame(result)\n","final"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xHuqr2nyQVJ2"},"source":["final[['full_text','top_keywords']]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-4OwKL9IRVW3"},"source":[""]},{"cell_type":"code","metadata":{"id":"o4_FEqJnQY1d"},"source":["## Abstract keyword  wordcloud ##\n","from wordcloud import WordCloud\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk import sent_tokenize, word_tokenize\n","from wordcloud import WordCloud"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XdqBYo_-RVp9"},"source":["from collections import Counter"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UI6O2wNzjgXD"},"source":["top_keyword = sum(final['top_keywords'],[])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cftNimDqi3el"},"source":["count = Counter(top_keyword)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qLnMs53SRhBz"},"source":["words = dict(count.most_common())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rHhrqceT7t1l"},"source":["### 단어 추천 ###"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2aPl05Sm7NB0"},"source":["import re\n","\n","import nltk\n","from nltk.corpus import stopwords \n","from nltk.tokenize import word_tokenize \n","\n","\n","stop_words=set(stopwords.words('english'))\n","nltk.download('punkt') # one time execution\n","nltk.download('stopwords')# one time execution"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FtivnYKm-BFP"},"source":[""]},{"cell_type":"code","metadata":{"id":"A6bTuRlx9KEd"},"source":["tokenized_data = []\n","for sentence in abstract:\n","    temp_X = nltk.word_tokenize(sentence) \n","    temp_X = [word for word in temp_X if not word in stop_words] \n","    tokenized_data.append(temp_X)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"r6K7-QHHkor6"},"source":["model_ft = FastText(tokenized_data, size=100, workers=4, sg=1, iter=6, word_ngrams=5)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xr2_uV3T-CzO"},"source":["model_ft_df = pd.DataFrame(model_ft.wv.most_similar(\"data\"), columns=['단어', '유사도'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ko4pGDYKArho"},"source":["model_ft_df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5DP-n8SzAwzR"},"source":[""],"execution_count":null,"outputs":[]}]}